{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ec095b-8540-428c-8c32-1b37629c19d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[INFO] Press 's' to start tracking and auto-capturing\n",
      "[INFO] Tracking started. Move around the person.\n",
      "[INFO] Tracking started. Move around the person.\n",
      "[INFO] Image capture complete. Generating point cloud...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "\n",
    "# Create folder to save captured images\n",
    "save_path = \"captured_images\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# For storing images\n",
    "captured_images = []\n",
    "max_images = 30\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"[INFO] Press 's' to start tracking and auto-capturing\")\n",
    "\n",
    "start_capture = False\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        if start_capture and len(captured_images) < max_images:\n",
    "            img_name = os.path.join(save_path, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(img_name, frame)\n",
    "            captured_images.append(img_name)\n",
    "            frame_count += 1\n",
    "            cv2.putText(frame, f\"Captured {len(captured_images)} / {max_images}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Camera Feed\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s'):\n",
    "        start_capture = True\n",
    "        print(\"[INFO] Tracking started. Move around the person.\")\n",
    "    elif key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"[INFO] Image capture complete. Generating point cloud...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4c6fc1-8750-417e-8ca8-57024dde1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Webcam live. Press 's' to start auto-capture of 30 images in 5 seconds.\n",
      "[INFO] Starting auto-capture...\n",
      "[INFO] Captured: colmap_images\\img_000.jpg\n",
      "[INFO] Captured: colmap_images\\img_001.jpg\n",
      "[INFO] Captured: colmap_images\\img_002.jpg\n",
      "[INFO] Captured: colmap_images\\img_003.jpg\n",
      "[INFO] Captured: colmap_images\\img_004.jpg\n",
      "[INFO] Captured: colmap_images\\img_005.jpg\n",
      "[INFO] Captured: colmap_images\\img_006.jpg\n",
      "[INFO] Captured: colmap_images\\img_007.jpg\n",
      "[INFO] Captured: colmap_images\\img_008.jpg\n",
      "[INFO] Captured: colmap_images\\img_009.jpg\n",
      "[INFO] Captured: colmap_images\\img_010.jpg\n",
      "[INFO] Captured: colmap_images\\img_011.jpg\n",
      "[INFO] Captured: colmap_images\\img_012.jpg\n",
      "[INFO] Captured: colmap_images\\img_013.jpg\n",
      "[INFO] Captured: colmap_images\\img_014.jpg\n",
      "[INFO] Captured: colmap_images\\img_015.jpg\n",
      "[INFO] Captured: colmap_images\\img_016.jpg\n",
      "[INFO] Captured: colmap_images\\img_017.jpg\n",
      "[INFO] Captured: colmap_images\\img_018.jpg\n",
      "[INFO] Captured: colmap_images\\img_019.jpg\n",
      "[INFO] Captured: colmap_images\\img_020.jpg\n",
      "[INFO] Captured: colmap_images\\img_021.jpg\n",
      "[INFO] Captured: colmap_images\\img_022.jpg\n",
      "[INFO] Captured: colmap_images\\img_023.jpg\n",
      "[INFO] Captured: colmap_images\\img_024.jpg\n",
      "[INFO] Captured: colmap_images\\img_025.jpg\n",
      "[INFO] Captured: colmap_images\\img_026.jpg\n",
      "[INFO] Captured: colmap_images\\img_027.jpg\n",
      "[INFO] Captured: colmap_images\\img_028.jpg\n",
      "[INFO] Captured: colmap_images\\img_029.jpg\n",
      "[✅] Done capturing images.\n",
      "[INFO] Running COLMAP (feature_extractor)...\n",
      "[INFO] Running COLMAP (exhaustive_matcher)...\n",
      "[INFO] Running COLMAP (mapper)...\n",
      "[❌] COLMAP failed. Check image quality and try again.\n",
      "[❌] Point cloud file (fused.ply) not found.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import open3d as o3d\n",
    "import time\n",
    "\n",
    "# === CONFIG ===\n",
    "image_dir = \"colmap_images\"\n",
    "output_dir = \"colmap_output\"\n",
    "num_images = 30\n",
    "capture_duration_sec = 5\n",
    "interval = capture_duration_sec / num_images\n",
    "colmap_bin = r\"C:\\Users\\vedhr\\CODES\\COLMAP\\bin\\colmap.exe\"  # <-- Update if needed\n",
    "\n",
    "# === CLEAN OLD FOLDERS ===\n",
    "if os.path.exists(image_dir):\n",
    "    shutil.rmtree(image_dir)\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(image_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# === STEP 1: WAIT FOR USER TO START AUTO-CAPTURE ===\n",
    "print(\"[INFO] Webcam live. Press 's' to start auto-capture of 30 images in 5 seconds.\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Could not access webcam.\")\n",
    "    exit()\n",
    "\n",
    "count = 0\n",
    "capture_started = False\n",
    "start_time = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Show instructions on the frame\n",
    "    display_text = \"Press 's' to start auto-capture\" if not capture_started else f\"Capturing: {count}/{num_images}\"\n",
    "    cv2.putText(frame, display_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Webcam Preview\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if not capture_started and key == ord('s'):\n",
    "        print(\"[INFO] Starting auto-capture...\")\n",
    "        capture_started = True\n",
    "        start_time = time.time()\n",
    "\n",
    "    if capture_started:\n",
    "        elapsed = time.time() - start_time\n",
    "        if count < num_images:\n",
    "            if elapsed >= count * interval:\n",
    "                filename = os.path.join(image_dir, f\"img_{count:03d}.jpg\")\n",
    "                cv2.imwrite(filename, frame)\n",
    "                print(f\"[INFO] Captured: {filename}\")\n",
    "                count += 1\n",
    "        else:\n",
    "            print(\"[✅] Done capturing images.\")\n",
    "            break\n",
    "\n",
    "    if key == ord('q'):\n",
    "        print(\"[INFO] Capture cancelled by user.\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        exit()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# === STEP 2: RUN COLMAP (CUDA ENABLED) ===\n",
    "database_path = os.path.join(output_dir, \"database.db\")\n",
    "sparse_path = os.path.join(output_dir, \"sparse\")\n",
    "dense_path = os.path.join(output_dir, \"dense\")\n",
    "fused_ply = os.path.join(dense_path, \"fused.ply\")\n",
    "\n",
    "def run_colmap():\n",
    "    print(\"[INFO] Running COLMAP (feature_extractor)...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"feature_extractor\",\n",
    "        \"--database_path\", database_path,\n",
    "        \"--image_path\", image_dir,\n",
    "        \"--ImageReader.single_camera\", \"1\",\n",
    "        \"--SiftExtraction.use_gpu\", \"1\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Running COLMAP (exhaustive_matcher)...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"exhaustive_matcher\",\n",
    "        \"--database_path\", database_path,\n",
    "        \"--SiftMatching.use_gpu\", \"1\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Running COLMAP (mapper)...\")\n",
    "    os.makedirs(sparse_path, exist_ok=True)\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"mapper\",\n",
    "        \"--database_path\", database_path,\n",
    "        \"--image_path\", image_dir,\n",
    "        \"--output_path\", sparse_path\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Running COLMAP (image_undistorter)...\")\n",
    "    os.makedirs(dense_path, exist_ok=True)\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"image_undistorter\",\n",
    "        \"--image_path\", image_dir,\n",
    "        \"--input_path\", os.path.join(sparse_path, \"0\"),\n",
    "        \"--output_path\", dense_path,\n",
    "        \"--output_type\", \"COLMAP\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Running COLMAP (patch_match_stereo)...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"patch_match_stereo\",\n",
    "        \"--workspace_path\", dense_path,\n",
    "        \"--workspace_format\", \"COLMAP\",\n",
    "        \"--PatchMatchStereo.geom_consistency\", \"true\",\n",
    "        \"--PatchMatchStereo.max_image_size\", \"2000\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Running COLMAP (stereo_fusion)...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"stereo_fusion\",\n",
    "        \"--workspace_path\", dense_path,\n",
    "        \"--workspace_format\", \"COLMAP\",\n",
    "        \"--input_type\", \"geometric\",\n",
    "        \"--output_path\", fused_ply\n",
    "    ], check=True)\n",
    "\n",
    "try:\n",
    "    run_colmap()\n",
    "    print(f\"[✅] Point cloud saved to: {fused_ply}\")\n",
    "except subprocess.CalledProcessError:\n",
    "    print(\"[❌] COLMAP failed. Check image quality and try again.\")\n",
    "    exit()\n",
    "\n",
    "# === STEP 3: VISUALIZE POINT CLOUD ===\n",
    "if os.path.exists(fused_ply):\n",
    "    print(\"[INFO] Opening point cloud viewer...\")\n",
    "    pcd = o3d.io.read_point_cloud(fused_ply)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "else:\n",
    "    print(\"[❌] Point cloud file (fused.ply) not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73c7d56-3d96-4a41-a9d5-fcc32b1c678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[INFO] Webcam live. Press 's' to start auto-capture of 30 images in 10 seconds.\n",
      "[INFO] Starting auto-capture...\n",
      "[INFO] Captured: colmap_images\\img_000.jpg\n",
      "[INFO] Captured: colmap_images\\img_001.jpg\n",
      "[INFO] Captured: colmap_images\\img_002.jpg\n",
      "[INFO] Captured: colmap_images\\img_003.jpg\n",
      "[INFO] Captured: colmap_images\\img_004.jpg\n",
      "[INFO] Captured: colmap_images\\img_005.jpg\n",
      "[INFO] Captured: colmap_images\\img_006.jpg\n",
      "[INFO] Captured: colmap_images\\img_007.jpg\n",
      "[INFO] Captured: colmap_images\\img_008.jpg\n",
      "[INFO] Captured: colmap_images\\img_009.jpg\n",
      "[INFO] Captured: colmap_images\\img_010.jpg\n",
      "[INFO] Captured: colmap_images\\img_011.jpg\n",
      "[INFO] Captured: colmap_images\\img_012.jpg\n",
      "[INFO] Captured: colmap_images\\img_013.jpg\n",
      "[INFO] Captured: colmap_images\\img_014.jpg\n",
      "[INFO] Captured: colmap_images\\img_015.jpg\n",
      "[INFO] Captured: colmap_images\\img_016.jpg\n",
      "[INFO] Captured: colmap_images\\img_017.jpg\n",
      "[INFO] Captured: colmap_images\\img_018.jpg\n",
      "[INFO] Captured: colmap_images\\img_019.jpg\n",
      "[INFO] Captured: colmap_images\\img_020.jpg\n",
      "[INFO] Captured: colmap_images\\img_021.jpg\n",
      "[INFO] Captured: colmap_images\\img_022.jpg\n",
      "[INFO] Captured: colmap_images\\img_023.jpg\n",
      "[INFO] Captured: colmap_images\\img_024.jpg\n",
      "[INFO] Captured: colmap_images\\img_025.jpg\n",
      "[INFO] Captured: colmap_images\\img_026.jpg\n",
      "[INFO] Captured: colmap_images\\img_027.jpg\n",
      "[INFO] Captured: colmap_images\\img_028.jpg\n",
      "[INFO] Captured: colmap_images\\img_029.jpg\n",
      "[✅] Done capturing.\n",
      "[INFO] Running feature extraction...\n",
      "[INFO] Matching features...\n",
      "[INFO] Building sparse map...\n",
      "[❌] COLMAP failed: Command '['C:\\\\Users\\\\vedhr\\\\CODES\\\\COLMAP\\\\bin\\\\colmap.exe', 'mapper', '--database_path', 'colmap_output\\\\database.db', '--image_path', 'colmap_images', '--output_path', 'colmap_output\\\\sparse']' returned non-zero exit status 1.\n",
      "[❌] Fused point cloud not found.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import open3d as o3d\n",
    "import time\n",
    "\n",
    "# === CONFIG ===\n",
    "image_dir = \"colmap_images\"\n",
    "output_dir = \"colmap_output\"\n",
    "num_images = 30\n",
    "capture_duration_sec = 10  # Increased to reduce blur\n",
    "interval = capture_duration_sec / num_images\n",
    "colmap_bin = r\"C:\\Users\\vedhr\\CODES\\COLMAP\\bin\\colmap.exe\"\n",
    "\n",
    "# === CLEANUP ===\n",
    "if os.path.exists(image_dir):\n",
    "    shutil.rmtree(image_dir)\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(image_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# === WEBCAM CAPTURE WITH TRIGGER ===\n",
    "print(f\"[INFO] Webcam live. Press 's' to start auto-capture of {num_images} images in {capture_duration_sec} seconds.\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Webcam could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "count = 0\n",
    "capture_started = False\n",
    "start_time = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"[ERROR] Frame capture failed.\")\n",
    "        break\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    cv2.putText(frame, f\"Resolution: {width}x{height}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1)\n",
    "\n",
    "    if not capture_started:\n",
    "        cv2.putText(frame, \"Press 's' to start auto-capture\", (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, f\"Capturing: {count}/{num_images}\", (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Live Feed\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if not capture_started and key == ord('s'):\n",
    "        print(\"[INFO] Starting auto-capture...\")\n",
    "        capture_started = True\n",
    "        start_time = time.time()\n",
    "\n",
    "    if capture_started:\n",
    "        elapsed = time.time() - start_time\n",
    "        if count < num_images and elapsed >= count * interval:\n",
    "            filename = os.path.join(image_dir, f\"img_{count:03d}.jpg\")\n",
    "            cv2.imwrite(filename, frame)\n",
    "            print(f\"[INFO] Captured: {filename}\")\n",
    "            count += 1\n",
    "        elif count >= num_images:\n",
    "            print(\"[✅] Done capturing.\")\n",
    "            break\n",
    "\n",
    "    if key == ord('q'):\n",
    "        print(\"[INFO] Quit by user.\")\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        exit()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# === VERIFY IMAGE COUNT ===\n",
    "actual_images = len(os.listdir(image_dir))\n",
    "if actual_images < 10:\n",
    "    print(f\"[❌] Only {actual_images} images captured. Need at least 10.\")\n",
    "    exit()\n",
    "\n",
    "# === COLMAP PATHS ===\n",
    "database_path = os.path.join(output_dir, \"database.db\")\n",
    "sparse_path = os.path.join(output_dir, \"sparse\")\n",
    "dense_path = os.path.join(output_dir, \"dense\")\n",
    "fused_ply = os.path.join(dense_path, \"fused.ply\")\n",
    "\n",
    "# === COLMAP PIPELINE ===\n",
    "def run_colmap():\n",
    "    print(\"[INFO] Running feature extraction...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"feature_extractor\",\n",
    "        \"--database_path\", database_path,\n",
    "        \"--image_path\", image_dir,\n",
    "        \"--ImageReader.single_camera\", \"1\",\n",
    "        \"--SiftExtraction.use_gpu\", \"1\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Matching features...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"exhaustive_matcher\",\n",
    "        \"--database_path\", database_path,\n",
    "        \"--SiftMatching.use_gpu\", \"1\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Building sparse map...\")\n",
    "    os.makedirs(sparse_path, exist_ok=True)\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"mapper\",\n",
    "        \"--database_path\", database_path,\n",
    "        \"--image_path\", image_dir,\n",
    "        \"--output_path\", sparse_path\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Undistorting images...\")\n",
    "    os.makedirs(dense_path, exist_ok=True)\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"image_undistorter\",\n",
    "        \"--image_path\", image_dir,\n",
    "        \"--input_path\", os.path.join(sparse_path, \"0\"),\n",
    "        \"--output_path\", dense_path,\n",
    "        \"--output_type\", \"COLMAP\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Running dense stereo...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"patch_match_stereo\",\n",
    "        \"--workspace_path\", dense_path,\n",
    "        \"--workspace_format\", \"COLMAP\",\n",
    "        \"--PatchMatchStereo.geom_consistency\", \"true\"\n",
    "    ], check=True)\n",
    "\n",
    "    print(\"[INFO] Fusing depth maps...\")\n",
    "    subprocess.run([\n",
    "        colmap_bin, \"stereo_fusion\",\n",
    "        \"--workspace_path\", dense_path,\n",
    "        \"--workspace_format\", \"COLMAP\",\n",
    "        \"--input_type\", \"geometric\",\n",
    "        \"--output_path\", fused_ply\n",
    "    ], check=True)\n",
    "\n",
    "try:\n",
    "    run_colmap()\n",
    "    print(f\"[✅] Point cloud saved to: {fused_ply}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"[❌] COLMAP failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "# === VISUALIZE RESULT ===\n",
    "if os.path.exists(fused_ply):\n",
    "    print(\"[INFO] Visualizing fused point cloud...\")\n",
    "    pcd = o3d.io.read_point_cloud(fused_ply)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "else:\n",
    "    print(\"[❌] Fused point cloud not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470fc5a-80f2-4629-978f-e3468b4559c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_realtime_gpu.py\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Import the model definitions from the project's files\n",
    "from model import Finetunemodel\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Parses and returns command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Real-Time SCI Enhancement on GPU\")\n",
    "    parser.add_argument('--model', type=str, default='./weights/medium.pt',\n",
    "                        help='Path to the pre-trained SCI model file (.pt)')\n",
    "    parser.add_argument('--webcam_id', type=int, default=0,\n",
    "                        help='ID of the webcam to use (usually 0)')\n",
    "    parser.add_argument('--width', type=int, default=640, help='Width of the webcam frame')\n",
    "    parser.add_argument('--height', type=int, default=480, help='Height of the webcam frame')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def preprocess_frame(frame, device):\n",
    "    \"\"\"Converts an OpenCV frame to a PyTorch tensor and moves it to the specified device.\"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_normalized = frame_rgb.astype(np.float32) / 255.0\n",
    "    frame_transposed = np.transpose(frame_normalized, (2, 0, 1))\n",
    "    tensor = torch.from_numpy(frame_transposed).unsqueeze(0)\n",
    "    # --- MODIFIED: Move the tensor to the GPU ---\n",
    "    return tensor.to(device)\n",
    "\n",
    "def postprocess_frame(tensor):\n",
    "    \"\"\"Converts a PyTorch tensor back to a displayable OpenCV frame (NumPy).\"\"\"\n",
    "    # .cpu() moves the tensor back to the CPU for NumPy and OpenCV operations\n",
    "    output = tensor.squeeze(0).cpu().detach().numpy()\n",
    "    output = np.transpose(output, (1, 2, 0))\n",
    "    output = np.clip(output * 255.0, 0, 255.0).astype(np.uint8)\n",
    "    frame_bgr = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "    return frame_bgr\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "\n",
    "    # --- ADDED: Device selection logic ---\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using device: CUDA (NVIDIA GPU)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using device: CPU\")\n",
    "\n",
    "    print(f\"Loading model: {args.model}\")\n",
    "    if not os.path.exists(args.model):\n",
    "        print(f\"Error: Model file not found at {args.model}\")\n",
    "        return\n",
    "\n",
    "    model = Finetunemodel(args.model)\n",
    "    # --- ADDED: Move the model to the GPU ---\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    cap = cv2.VideoCapture(args.webcam_id)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open webcam with ID {args.webcam_id}.\")\n",
    "        return\n",
    "        \n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, args.width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, args.height)\n",
    "    print(\"Webcam initialized. Press 'q' in the display window to quit.\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Failed to grab frame.\")\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # --- MODIFIED: Pass the device to the preprocessing function ---\n",
    "            input_tensor = preprocess_frame(frame, device)\n",
    "            \n",
    "            # The model and tensor are both on the GPU, so this runs on the GPU\n",
    "            _, enhanced_tensor = model(input_tensor)\n",
    "            \n",
    "            enhanced_frame = postprocess_frame(enhanced_tensor)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            fps = 1 / (end_time - start_time) if (end_time - start_time) > 0 else 0\n",
    "\n",
    "            cv2.putText(enhanced_frame, f\"FPS: {int(fps)}\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            combined_frame = np.hstack((frame, enhanced_frame))\n",
    "            cv2.imshow('Original vs. SCI Enhanced (GPU)', combined_frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    print(\"Exiting...\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (GPU)",
   "language": "python",
   "name": "pytorch-gpu-2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
